{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 14084,
          "sourceType": "datasetVersion",
          "datasetId": 9961
        }
      ],
      "dockerImageVersionId": 30626,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Big Mart Sales Prediction",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wardasidd/MyDataScienceProjects/blob/main/Big_Mart_Sales_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'bigmart-sales-data:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F9961%2F14084%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240825%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240825T221045Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8e6b57002ceff3d1d659597e4f9f1d3ea9946d86a7e91507169845e07e0de6faa1f5808e7158dcb6cbb09eb8ba7967a9ca7720252224cf43de78911905cba4842780864c17cbd7ae1c2b4b3374509bc8d0059fbf6b2dc34482f739c9c3dcae3f3f8feeefc47b5324ee65aca0eb5168ed52608f73244949e1ae18838fd7532880b5ca6204dffd38d33d62eaa11d7b85f2414bfe53960c805db39317b85f59f5983f3ca9aba7ad7c4c2dac13df0a1f73ef4ea66022c5d53048890e1397dba80c8056d6240a7470e355b377c6fb4655df29fe9bf22c902afba22be39758e1a9706964771dd26da9fdc18da2e381b3583469508f85849fcfd5c051bda358ab4faaa9'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "PLnoRPIfl4pd"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective:\n",
        "To build a predictive model that can find out the sales of each product at a particular store and then provide actionable recommendations to the BigMart sales team to understand the properties of products and stores which play a key role in increasing sales"
      ],
      "metadata": {
        "id": "Z9h0VM0Ol4pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as ex\n",
        "import plotly.graph_objects as gb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:22.834829Z",
          "iopub.execute_input": "2024-01-03T20:36:22.835205Z",
          "iopub.status.idle": "2024-01-03T20:36:22.841654Z",
          "shell.execute_reply.started": "2024-01-03T20:36:22.835176Z",
          "shell.execute_reply": "2024-01-03T20:36:22.840188Z"
        },
        "trusted": true,
        "id": "ejK563cBl4ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read train and test data\n",
        "train_df=pd.read_csv('/kaggle/input/bigmart-sales-data/Train.csv')\n",
        "test_df=pd.read_csv('/kaggle/input/bigmart-sales-data/Test.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:22.844032Z",
          "iopub.execute_input": "2024-01-03T20:36:22.844535Z",
          "iopub.status.idle": "2024-01-03T20:36:22.886992Z",
          "shell.execute_reply.started": "2024-01-03T20:36:22.844495Z",
          "shell.execute_reply": "2024-01-03T20:36:22.885908Z"
        },
        "trusted": true,
        "id": "UJtHHPeYl4ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the first five rows of the dataset\n",
        "train_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:22.888525Z",
          "iopub.execute_input": "2024-01-03T20:36:22.888965Z",
          "iopub.status.idle": "2024-01-03T20:36:22.906526Z",
          "shell.execute_reply.started": "2024-01-03T20:36:22.888926Z",
          "shell.execute_reply": "2024-01-03T20:36:22.905279Z"
        },
        "trusted": true,
        "id": "0_l2zWyAl4ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation:\n",
        "As Item_Identifier and Outlet_identifier both are just for identity pupose and do not have any contribution in predicting the target variable (Item_Outlet_Sales) so we will be dropping these columns\n"
      ],
      "metadata": {
        "id": "kbfhkDoul4pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping the columns from both datasets\n",
        "train_df=train_df.drop([\"Outlet_Identifier\",\"Item_Identifier\"],axis=1) #axis=1 represents column\n",
        "test_df=test_df.drop([\"Outlet_Identifier\",\"Item_Identifier\"],axis=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:22.908552Z",
          "iopub.execute_input": "2024-01-03T20:36:22.908864Z",
          "iopub.status.idle": "2024-01-03T20:36:22.918315Z",
          "shell.execute_reply.started": "2024-01-03T20:36:22.908838Z",
          "shell.execute_reply": "2024-01-03T20:36:22.917257Z"
        },
        "trusted": true,
        "id": "AiWkjt3xl4pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To understand more about each data frame such as non-null values and Dtype\n",
        "train_df.info()\n",
        "test_df.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:22.919537Z",
          "iopub.execute_input": "2024-01-03T20:36:22.91996Z",
          "iopub.status.idle": "2024-01-03T20:36:22.943747Z",
          "shell.execute_reply.started": "2024-01-03T20:36:22.919931Z",
          "shell.execute_reply": "2024-01-03T20:36:22.942739Z"
        },
        "trusted": true,
        "id": "Uf4ZRHLvl4pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Train dataset has total **8523** entries whereas test dataset has **5681** entries.\n",
        "2. In both dataset **Item_Weight and Outlet_Size** has missing valus as no. of non-null values are less than the total rows or entries.\n",
        "3. There are in total five coloumns with DType:Objest and those are **Outlet_Size, Outlet_Location_Type,Outlet_Type,Item_Type and Item_Fat_Content**\n",
        "indicating these are strings or categorial valriables."
      ],
      "metadata": {
        "id": "bTUlVhB4l4pi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA PREPROCESSING AND EXPLORATORY DATA ANALYSIS**"
      ],
      "metadata": {
        "id": "jfLpOe7Bl4pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Starting with visualizing the categorial variables\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize = (18, 12))\n",
        "\n",
        "fig.suptitle('Bar plot for all categorical variables in the dataset')\n",
        "\n",
        "sns.countplot(ax = axes[0, 0], x = 'Item_Fat_Content', data = train_df, color = 'green',\n",
        "              order = train_df['Item_Fat_Content'].value_counts().index);\n",
        "\n",
        "sns.countplot(ax = axes[0, 1], x = 'Outlet_Size', data = train_df, color = 'black',\n",
        "              order = train_df['Outlet_Size'].value_counts().index);\n",
        "\n",
        "sns.countplot(ax = axes[1, 0], x = 'Outlet_Location_Type', data = train_df, color = 'red',\n",
        "              order = train_df['Outlet_Location_Type'].value_counts().index);\n",
        "\n",
        "sns.countplot(ax = axes[1, 1], x = 'Outlet_Type', data = train_df, color = 'blue',\n",
        "              order = train_df['Outlet_Type'].value_counts().index);"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:22.94526Z",
          "iopub.execute_input": "2024-01-03T20:36:22.945557Z",
          "iopub.status.idle": "2024-01-03T20:36:23.649721Z",
          "shell.execute_reply.started": "2024-01-03T20:36:22.94553Z",
          "shell.execute_reply": "2024-01-03T20:36:23.648714Z"
        },
        "trusted": true,
        "id": "pedYDu7kl4pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation:\n",
        "\n",
        "1. By visulaizing **\"Item_Fat_Content\"**, we can clearly see that the Low Fat is also written as LF and low fat,whereas Regular is also written as reg.So this issue needs to be fixed.\n",
        "2. **Outlet_Size** is categorized into Medium, Small and High Whereas **Outlet_Location_Type** is Classified into Tier 3 , Tier 2 and Tier 1.\n",
        "3. In the **column Outlet_Type**, the majority of the stores are of Supermarket Type 1 and we have a less and almost equal number of representatives in the other categories including Supermarket Type 2, Supermarket Type 3, and Grocery Store"
      ],
      "metadata": {
        "id": "Q9kLFXXUl4pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Visulaizing Item_Type - Categorial variable.\n",
        "fig = plt.figure(figsize = (18, 6))\n",
        "\n",
        "sns.countplot(x = 'Item_Type', data = train_df, color = 'teal', order = train_df['Item_Type'].value_counts().index);\n",
        "\n",
        "plt.xticks(rotation = 45);"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:23.651288Z",
          "iopub.execute_input": "2024-01-03T20:36:23.651711Z",
          "iopub.status.idle": "2024-01-03T20:36:23.998531Z",
          "shell.execute_reply.started": "2024-01-03T20:36:23.651646Z",
          "shell.execute_reply": "2024-01-03T20:36:23.99777Z"
        },
        "trusted": true,
        "id": "Vq-mNjJil4pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation:\n",
        "By Visualizing **Item_Type**, we observe that majority of the items sold in these stores are Fruits and Vegetables, followed by Snack Foods and Household items."
      ],
      "metadata": {
        "id": "oEECS-Dnl4pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before moving forward, Let's fix the issue we encountered in Item_Fat_content.\n",
        "#Don't forget to fix it in both train and test data set\n",
        "train_df['Item_Fat_Content'] = train_df['Item_Fat_Content'].apply(lambda x: 'Low Fat' if x == 'low fat' or x == 'LF' else x)\n",
        "train_df['Item_Fat_Content'] = train_df['Item_Fat_Content'].apply(lambda x: 'Regular' if x == 'reg' else x)\n",
        "test_df['Item_Fat_Content'] = train_df['Item_Fat_Content'].apply(lambda x: 'Low Fat' if x == 'low fat' or x == 'LF' else x)\n",
        "test_df['Item_Fat_Content'] = train_df['Item_Fat_Content'].apply(lambda x: 'Regular' if x == 'reg' else x)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:23.999677Z",
          "iopub.execute_input": "2024-01-03T20:36:24.00039Z",
          "iopub.status.idle": "2024-01-03T20:36:24.018709Z",
          "shell.execute_reply.started": "2024-01-03T20:36:24.000355Z",
          "shell.execute_reply": "2024-01-03T20:36:24.017848Z"
        },
        "trusted": true,
        "id": "UZkJwlZBl4pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now Visulaizing Numerical Variables\n",
        "fig, axes = plt.subplots(1, 3, figsize = (20, 6))\n",
        "fig.suptitle('Histogram for all numerical variables')\n",
        "sns.histplot(x = 'Item_Weight', data = train_df, kde = True, ax = axes[0]);\n",
        "sns.histplot(x = 'Item_Visibility', data = train_df, kde = True, ax = axes[1],color='purple');\n",
        "sns.histplot(x = 'Item_MRP', data = train_df, kde = True, ax = axes[2],color='pink' );"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:24.021685Z",
          "iopub.execute_input": "2024-01-03T20:36:24.022671Z",
          "iopub.status.idle": "2024-01-03T20:36:25.055844Z",
          "shell.execute_reply.started": "2024-01-03T20:36:24.022638Z",
          "shell.execute_reply": "2024-01-03T20:36:25.054779Z"
        },
        "trusted": true,
        "id": "XyWDVeknl4pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation:\n",
        "1. The **'Item_Weight'** variable exhibits an approximate uniform distribution. When addressing the missing values in this column, it's essential to ensure that the imputation does not substantially alter the original distribution.\n",
        "\n",
        "2. On the other hand, the **'Item_Visibility'** variable is characterized by a right-skewed distribution. This indicates that while most items have a lower percentage of display area, a few items significantly exceed the average, occupying a much larger display area.\n",
        "\n",
        "3. Lastly, the **'Item_MRP'** variable appears to follow a roughly multi-modal normal distribution. This suggests that the maximum retail price of items might cluster around several different values, rather than centering around a single mean."
      ],
      "metadata": {
        "id": "hOyCUVJkl4pk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bivariate Analysis"
      ],
      "metadata": {
        "id": "mNYAO4fUl4pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets move ahead with visualizing relationship between different variables\n",
        "\n",
        "#Visualizing Relationship between Outlet_Establishment_Year & Item_Outlet_Sales\n",
        "\n",
        "fig = plt.figure(figsize = (16, 5))\n",
        "sns.lineplot(x = 'Outlet_Establishment_Year', y = 'Item_Outlet_Sales', data = train_df, errorbar=None, estimator = 'mean');"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:25.057092Z",
          "iopub.execute_input": "2024-01-03T20:36:25.057903Z",
          "iopub.status.idle": "2024-01-03T20:36:25.309621Z",
          "shell.execute_reply.started": "2024-01-03T20:36:25.057871Z",
          "shell.execute_reply": "2024-01-03T20:36:25.308715Z"
        },
        "trusted": true,
        "id": "qoe_CQuUl4pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation:\n",
        "It's observed that the average sales have remained relatively stable annually, indicating no discernible upward or downward trend over time. Consequently, the year variable may not serve as a reliable predictor for forecasting sales, a hypothesis that can be further examined during the modeling stage.\n",
        "\n",
        "Additionally, there's a notable dip in average sales in 1998. This sudden decrease might be attributed to external influences not captured within the dataset."
      ],
      "metadata": {
        "id": "QqsiJxLzl4pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To visulaize relationship between every variable\n",
        "numeric_df = train_df.select_dtypes(include=[np.number])\n",
        "# Calculate the correlation matrix\n",
        "corr = numeric_df.corr()\n",
        "# Set up the matplotlib figure\n",
        "fig = plt.figure(figsize=(18, 6))\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, annot=True)\n",
        "# Rotate the x-axis labels\n",
        "plt.xticks(rotation=45)\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:25.311055Z",
          "iopub.execute_input": "2024-01-03T20:36:25.311379Z",
          "iopub.status.idle": "2024-01-03T20:36:25.656657Z",
          "shell.execute_reply.started": "2024-01-03T20:36:25.311351Z",
          "shell.execute_reply": "2024-01-03T20:36:25.655681Z"
        },
        "trusted": true,
        "id": "ZFfXTk37l4pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation:\n",
        "\n",
        "Based on the visualization provided, it appears that **'Item_MRP'** (Maximum Retail Price) is the sole independent variable displaying a moderate linear association with the dependent variable, **'Item_Outlet_Sales'**. This suggests a potential trend where changes in 'Item_MRP' might correspond to changes in 'Item_Outlet_Sales', although the relationship is not particularly robust.\n",
        "\n",
        "As for the other variables, they don't exhibit any marked positive or negative correlations with the dependent variable. This indicates that they may not significantly influence or predict the behavior of 'Item_Outlet_Sales' when considering linear relationships alone. Further analysis might be required to uncover more complex or subtle relationships within the data"
      ],
      "metadata": {
        "id": "3_bYAPQ1l4pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#scatter-plot of all Numercical variables with dependent variable\n",
        "fig, axes = plt.subplots(1, 3, figsize = (20, 6))\n",
        "fig.suptitle('Bi-variate scatterplot for all numerical variables with the dependent variable')\n",
        "sns.scatterplot(x = 'Item_Weight', y = 'Item_Outlet_Sales', data = train_df, ax = axes[0]);\n",
        "sns.scatterplot(x = 'Item_Visibility', y = 'Item_Outlet_Sales', data = train_df, ax = axes[1]);\n",
        "sns.scatterplot(x = 'Item_MRP', y = 'Item_Outlet_Sales', data = train_df, ax = axes[2]);"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:25.658126Z",
          "iopub.execute_input": "2024-01-03T20:36:25.658879Z",
          "iopub.status.idle": "2024-01-03T20:36:26.457292Z",
          "shell.execute_reply.started": "2024-01-03T20:36:25.658848Z",
          "shell.execute_reply": "2024-01-03T20:36:26.456284Z"
        },
        "trusted": true,
        "id": "dGtxam8ql4pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation:\n",
        "\n",
        "The initial scatter plot illustrates a lack of discernible relationship between **'Item_Weight'** and **'Item_Outlet_Sales'**, indicating the data points are scattered randomly. This observation aligns with the previously noted weak correlation between these two variables, suggesting that **'Item_Weight'** might not be a significant predictor of **'Item_Outlet_Sales'.**\n",
        "\n",
        "In the second scatter plot, examining **'Item_Visibility'** against **'Item_Outlet_Sales'**, no strong linear relationship is evident. However, a trend is observed where sales tend to decrease as **'Item_Visibility'** exceeds 0.19. This pattern might imply that items not typically sold frequently are allocated more visibility, possibly under the assumption that increased visibility would boost sales. This insight could lead to the creation of new features in the future, such as categorizing items into 'high visibility' and 'low visibility' groups, although this isn't pursued in the current analysis.\n",
        "\n",
        "The third scatter plot, featuring **'Item_MRP'** versus **'Item_Outlet_Sales'**, distinctly shows a positive correlation. This indicates that as **'Item_MRP'** increases, **'Item_Outlet_Sales'** tend to increase as well, signifying that **'Item_MRP'** could be a strong predictor for sales and an important variable for future modeling efforts."
      ],
      "metadata": {
        "id": "r9_4oVDXl4pl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing Value Treatment"
      ],
      "metadata": {
        "id": "KEptjFIjl4pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check for missing values\n",
        "train_df.isnull().sum()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:26.458761Z",
          "iopub.execute_input": "2024-01-03T20:36:26.459183Z",
          "iopub.status.idle": "2024-01-03T20:36:26.473372Z",
          "shell.execute_reply.started": "2024-01-03T20:36:26.459136Z",
          "shell.execute_reply": "2024-01-03T20:36:26.472223Z"
        },
        "trusted": true,
        "id": "nSIt0tY3l4pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above result we can observe that **Item_Weight** and **Outlet_size** have 1463 and 2410 missing values respectively\n",
        "\n",
        "Lets treat **Item_Weight** first:"
      ],
      "metadata": {
        "id": "9jr1wkEHl4pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Visulaizing each Item type's average Fat content\n",
        "\n",
        "fig = plt.figure(figsize = (18, 3))\n",
        "sns.heatmap(train_df.pivot_table(index = 'Item_Fat_Content', columns = 'Item_Type', values = 'Item_Weight'), annot = True);\n",
        "plt.xticks(rotation = 45);"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:26.474789Z",
          "iopub.execute_input": "2024-01-03T20:36:26.47538Z",
          "iopub.status.idle": "2024-01-03T20:36:26.986105Z",
          "shell.execute_reply.started": "2024-01-03T20:36:26.475339Z",
          "shell.execute_reply": "2024-01-03T20:36:26.984781Z"
        },
        "trusted": true,
        "id": "gKhidCJbl4pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation:\n",
        "In the above heatmap,Different combinations of **Item_Types** and **Item_Fat_Content**, the average range of values for the column **Item_Weight** lies between the minimum value of 10 and the maximum value of 14."
      ],
      "metadata": {
        "id": "M5XKtgSNl4pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Visulaizing Item_Weight with respect to Outlet_type and Location\n",
        "fig = plt.figure(figsize = (18, 3))\n",
        "sns.heatmap(train_df.pivot_table(index = 'Outlet_Type', columns = 'Outlet_Location_Type', values = 'Item_Weight'), annot = True);"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:26.987541Z",
          "iopub.execute_input": "2024-01-03T20:36:26.987968Z",
          "iopub.status.idle": "2024-01-03T20:36:27.312779Z",
          "shell.execute_reply.started": "2024-01-03T20:36:26.987927Z",
          "shell.execute_reply": "2024-01-03T20:36:27.311542Z"
        },
        "trusted": true,
        "id": "flYmDx3al4pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As observed the average value for each location and Outlet_Type is 13"
      ],
      "metadata": {
        "id": "1gqYKdTll4pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Item_weight with respect to Outlet_size\n",
        "fig = plt.figure(figsize = (18, 3))\n",
        "sns.heatmap(train_df.pivot_table(index = 'Item_Fat_Content', columns = 'Outlet_Size', values = 'Item_Weight'), annot = True);"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:27.314288Z",
          "iopub.execute_input": "2024-01-03T20:36:27.314736Z",
          "iopub.status.idle": "2024-01-03T20:36:27.630144Z",
          "shell.execute_reply.started": "2024-01-03T20:36:27.31468Z",
          "shell.execute_reply": "2024-01-03T20:36:27.629298Z"
        },
        "trusted": true,
        "id": "JLcqokxIl4pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is also constant at 13 for every category."
      ],
      "metadata": {
        "id": "lQpHW1oAl4pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imputing Missing values in Item_Weight\n",
        "#Randomnly Imputing from a uniform distribution over the interval [10, 14).\n",
        "\n",
        "#For train dataset\n",
        "item_weight_updated = train_df[train_df['Item_Weight'].isnull()].index\n",
        "train_df.loc[item_weight_updated, 'Item_Weight'] = np.random.uniform(10, 14, len(item_weight_updated));\n",
        "\n",
        "#For test dataset\n",
        "item_weight_updated = test_df[test_df['Item_Weight'].isnull()].index\n",
        "test_df.loc[item_weight_updated, 'Item_Weight'] = np.random.uniform(10, 14, len(item_weight_updated));\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:27.631529Z",
          "iopub.execute_input": "2024-01-03T20:36:27.632094Z",
          "iopub.status.idle": "2024-01-03T20:36:27.640823Z",
          "shell.execute_reply.started": "2024-01-03T20:36:27.632064Z",
          "shell.execute_reply": "2024-01-03T20:36:27.639836Z"
        },
        "trusted": true,
        "id": "WdXScmwLl4pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outlet_size_data = train_df[train_df['Outlet_Size'].notnull()]\n",
        "outlet_size_missing_data = train_df[train_df['Outlet_Size'].isnull()]\n",
        "fig, axes = plt.subplots(1, 3, figsize = (18, 6))\n",
        "fig.suptitle('Bar plot for all categorical variables in the dataset where the variable Outlet_Size is missing')\n",
        "sns.countplot(ax = axes[0], x = 'Outlet_Type', data = outlet_size_missing_data, color = 'teal',\n",
        "              order = outlet_size_missing_data['Outlet_Type'].value_counts().index);\n",
        "sns.countplot(ax = axes[1], x = 'Outlet_Location_Type', data = outlet_size_missing_data, color = 'blue',\n",
        "              order = outlet_size_missing_data['Outlet_Location_Type'].value_counts().index)\n",
        "sns.countplot(ax = axes[2], x = 'Item_Fat_Content', data = outlet_size_missing_data, color = 'brown',\n",
        "              order = outlet_size_missing_data['Item_Fat_Content'].value_counts().index);"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:27.642316Z",
          "iopub.execute_input": "2024-01-03T20:36:27.642674Z",
          "iopub.status.idle": "2024-01-03T20:36:28.441155Z",
          "shell.execute_reply.started": "2024-01-03T20:36:27.642645Z",
          "shell.execute_reply": "2024-01-03T20:36:28.440109Z"
        },
        "trusted": true,
        "id": "onRu1Bu0l4pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation:\n",
        "\n",
        "Wherever Outlet_Size is missing in the dataset, most of the values are **Outlet_Type** as Supermarket Type 1, **Outlet_Location_Type** as Tier 2, and **Item_Fat_Content** as Low Fat."
      ],
      "metadata": {
        "id": "Tx8978Lxl4pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#understanding and visualizing non-null values in Outlet_size w.r.t Outlet_type\n",
        "fig= plt.figure(figsize = (18, 3))\n",
        "sns.heatmap(pd.crosstab(index = outlet_size_data['Outlet_Type'], columns = outlet_size_data['Outlet_Size']), annot = True,fmt='g')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:28.442634Z",
          "iopub.execute_input": "2024-01-03T20:36:28.443009Z",
          "iopub.status.idle": "2024-01-03T20:36:28.801078Z",
          "shell.execute_reply.started": "2024-01-03T20:36:28.442979Z",
          "shell.execute_reply": "2024-01-03T20:36:28.80001Z"
        },
        "trusted": true,
        "id": "vfW6qayHl4pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation:\n",
        "\n",
        "From the heatmap analysis, it's apparent that grocery stores uniformly correspond to a 'Small' **Outlet_Size**.\n",
        "\n",
        "Additionally, it's consistently observed that both Supermarket Type 2 and Supermarket Type 3 are categorized under a 'Medium' outlet size."
      ],
      "metadata": {
        "id": "bz38cpY5l4pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing Outlet_size w.r.t Outlet_Location_Type\n",
        "fig = plt.figure(figsize = (18, 3))\n",
        "sns.heatmap(pd.crosstab(index = outlet_size_data['Outlet_Location_Type'], columns = outlet_size_data['Outlet_Size']), annot = True, fmt = 'g')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:28.802633Z",
          "iopub.execute_input": "2024-01-03T20:36:28.803143Z",
          "iopub.status.idle": "2024-01-03T20:36:29.158104Z",
          "shell.execute_reply.started": "2024-01-03T20:36:28.803104Z",
          "shell.execute_reply": "2024-01-03T20:36:29.15692Z"
        },
        "trusted": true,
        "id": "KmQljtEll4pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation:\n",
        "\n",
        "As observed all the Tier2 locations are \"small\"."
      ],
      "metadata": {
        "id": "q3Qg-FcUl4pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing Outlet_Size w.r.t Item_Fat_Content\n",
        "fig = plt.figure(figsize = (18, 3))\n",
        "sns.heatmap(pd.crosstab(index = outlet_size_data['Item_Fat_Content'], columns = outlet_size_data['Outlet_Size']), annot = True, fmt = 'g')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:29.15939Z",
          "iopub.execute_input": "2024-01-03T20:36:29.160239Z",
          "iopub.status.idle": "2024-01-03T20:36:29.48004Z",
          "shell.execute_reply.started": "2024-01-03T20:36:29.160208Z",
          "shell.execute_reply": "2024-01-03T20:36:29.478959Z"
        },
        "trusted": true,
        "id": "HPvbxclNl4pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation:\n",
        "\n",
        "From the heatmap provided, There's no evident pattern among **'Item_Fat_Content'** and **'Outlet_Size'**."
      ],
      "metadata": {
        "id": "4k5zme6el4pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imputing Outlet_size='Small' where Outlet_Type='Grocery store' and Outlet_Location_Type ='Tier 2'\n",
        "grocery_store= train_df[train_df['Outlet_Size'].isnull()].query(\" Outlet_Type == 'Grocery Store' \").index\n",
        "tier_2= train_df[train_df['Outlet_Size'].isnull()].query(\" Outlet_Location_Type == 'Tier 2' \").index\n",
        "train_df.loc[grocery_store, 'Outlet_Size'] = 'Small'\n",
        "train_df.loc[tier_2, 'Outlet_Size'] = 'Small'\n",
        "\n",
        "#Performing same on Test data\n",
        "grocery_store= test_df[test_df['Outlet_Size'].isnull()].query(\" Outlet_Type == 'Grocery Store' \").index\n",
        "tier_2= test_df[test_df['Outlet_Size'].isnull()].query(\" Outlet_Location_Type == 'Tier 2' \").index\n",
        "test_df.loc[grocery_store, 'Outlet_Size'] = 'Small'\n",
        "test_df.loc[tier_2, 'Outlet_Size'] = 'Small'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:29.481674Z",
          "iopub.execute_input": "2024-01-03T20:36:29.482332Z",
          "iopub.status.idle": "2024-01-03T20:36:29.509902Z",
          "shell.execute_reply.started": "2024-01-03T20:36:29.482292Z",
          "shell.execute_reply": "2024-01-03T20:36:29.508949Z"
        },
        "trusted": true,
        "id": "68hBuP1Ml4pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check for null values again in train and test datasets\n",
        "train_df.isnull().sum()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:29.510886Z",
          "iopub.execute_input": "2024-01-03T20:36:29.511176Z",
          "iopub.status.idle": "2024-01-03T20:36:29.521911Z",
          "shell.execute_reply.started": "2024-01-03T20:36:29.511152Z",
          "shell.execute_reply": "2024-01-03T20:36:29.520844Z"
        },
        "trusted": true,
        "id": "IlKk6dpYl4pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.isnull().sum()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:29.527923Z",
          "iopub.execute_input": "2024-01-03T20:36:29.528374Z",
          "iopub.status.idle": "2024-01-03T20:36:29.538687Z",
          "shell.execute_reply.started": "2024-01-03T20:36:29.528343Z",
          "shell.execute_reply": "2024-01-03T20:36:29.537739Z"
        },
        "trusted": true,
        "id": "Qg3vvRKBl4pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering:\n",
        "\n",
        "Having completed the steps of data understanding and preparation, we're now poised to advance to the modeling phase. At this juncture, it's recognized that while certain predictive features aren't directly present in the dataset, they could be ingeniously derived from the existing columns. This process of innovating new predictors from existing data attributes is referred to as **Feature Engineering**.\n",
        "\n",
        "Embarking on this path, we hypothesize that a store's age might influence its sales, speculating that older stores could potentially have higher sales. To operationalize 'age', we'll calculate **'Outlet_Age'** by deducting the establishment year from 2013, the year when this data was compiled. This newly crafted feature, 'Outlet_Age', will thus reflect the respective ages of the outlets, and is introduced in the dataset through the following code."
      ],
      "metadata": {
        "id": "MOY34jnvl4pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Visulaizing the Hypothesis by plotting Outlet Sales by Outlet age\n",
        "train_df['Outlet_Age'] = 2013 - train_df['Outlet_Establishment_Year']\n",
        "test_df['Outlet_Age'] = 2013 - test_df['Outlet_Establishment_Year']\n",
        "\n",
        "average_sales_by_age = train_df.groupby('Outlet_Age')['Item_Outlet_Sales'].mean()\n",
        "\n",
        "plt.figure(figsize=(18, 6))\n",
        "average_sales_by_age.plot(kind='line')\n",
        "plt.title('Average Item Outlet Sales by Outlet Age')\n",
        "plt.xlabel('Outlet Age')\n",
        "plt.ylabel('Average Item Outlet Sales')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:29.539987Z",
          "iopub.execute_input": "2024-01-03T20:36:29.540301Z",
          "iopub.status.idle": "2024-01-03T20:36:29.843579Z",
          "shell.execute_reply.started": "2024-01-03T20:36:29.540273Z",
          "shell.execute_reply": "2024-01-03T20:36:29.842782Z"
        },
        "trusted": true,
        "id": "GBbt4ED7l4pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation:\n",
        "Based on the visualizations produced, our initial hypothesis — that older stores would generally report higher sales — doesn't appear to be supported. The sales distributions across stores of varying ages seem to be roughly similar, indicating that store age may not be as significant a predictor of sales as initially presumed. However, we'll retain the 'Outlet_Age' feature for the time being. Its true impact and relevance will be more thoroughly assessed during the model-building phase. At that point, we can make a more informed decision about whether to keep or discard this variable, depending on its statistical significance and contribution to the model's performance."
      ],
      "metadata": {
        "id": "XH6j34kSl4pp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL"
      ],
      "metadata": {
        "id": "1hnBqeFdl4pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the target variable from the feature set\n",
        "# Removing 'Outlet_Establishment_Year', as we have created a new variable 'Outlet_Age'\n",
        "X_train = train_df.drop(['Item_Outlet_Sales', 'Outlet_Establishment_Year'], axis = 1)\n",
        "# And then we are extracting the outcome variable separately\n",
        "Y_train = train_df['Item_Outlet_Sales']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:29.844843Z",
          "iopub.execute_input": "2024-01-03T20:36:29.845347Z",
          "iopub.status.idle": "2024-01-03T20:36:29.851348Z",
          "shell.execute_reply.started": "2024-01-03T20:36:29.845316Z",
          "shell.execute_reply": "2024-01-03T20:36:29.850249Z"
        },
        "trusted": true,
        "id": "bifmJIGLl4pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dummy variables for the categorical variables\n",
        "X_train = pd.get_dummies(X_train, drop_first = True)\n",
        "X_train.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:29.852518Z",
          "iopub.execute_input": "2024-01-03T20:36:29.852968Z",
          "iopub.status.idle": "2024-01-03T20:36:29.892304Z",
          "shell.execute_reply.started": "2024-01-03T20:36:29.852935Z",
          "shell.execute_reply": "2024-01-03T20:36:29.89155Z"
        },
        "trusted": true,
        "id": "s3ZiKYYel4pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an instance of the MinMaxScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "# Applying fit_transform on the training features data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "# The above scaler returns the data in array format, below we are converting it back to pandas DataFrame\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, index = X_train.index, columns = X_train.columns)\n",
        "X_train_scaled.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:29.893516Z",
          "iopub.execute_input": "2024-01-03T20:36:29.894083Z",
          "iopub.status.idle": "2024-01-03T20:36:29.935508Z",
          "shell.execute_reply.started": "2024-01-03T20:36:29.894049Z",
          "shell.execute_reply": "2024-01-03T20:36:29.934498Z"
        },
        "trusted": true,
        "id": "SvYGJGS0l4pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "# Adding the intercept term\n",
        "X_train_scaled = sm.add_constant(X_train_scaled)\n",
        "# Calling the OLS algorithm on the train features and the target variable\n",
        "ols_model_1 = sm.OLS(Y_train, X_train_scaled)\n",
        "# Fitting the Model\n",
        "ols_1 = ols_model_1.fit()\n",
        "#Printing the summary of the model\n",
        "print(ols_1.summary())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:29.936817Z",
          "iopub.execute_input": "2024-01-03T20:36:29.937133Z",
          "iopub.status.idle": "2024-01-03T20:36:30.001241Z",
          "shell.execute_reply.started": "2024-01-03T20:36:29.937106Z",
          "shell.execute_reply": "2024-01-03T20:36:30.00015Z"
        },
        "trusted": true,
        "id": "Ujmxxg8Il4pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation:\n",
        "\n",
        "The R-squared value for our model stands at 0.563, indicating the proportion of variance in the dependent variable that's predictable from the independent variables. However, not all predictors are statistically significant in forecasting the outcome variable. To discern which variables hold predictive power, we need to examine the p-values associated with each independent variable."
      ],
      "metadata": {
        "id": "lCbei0fpl4pq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpreting the Regression Results:\n",
        "\n",
        "**Adj. R-squared:** This metric adjusts the R-squared value based on the number of predictors and the sample size, providing a more accurate measure of model fit. Values range from 0 to 1, where higher values typically denote a better model fit. Here, the Adjusted R-squared is 0.562, suggesting that around 56.2% of the variability in the dependent variable is explained by the model.\n",
        "\n",
        "**Coefficients:** These values indicate the expected change in the dependent variable for a one-unit change in the predictor, holding all other predictors constant.\n",
        "\n",
        "**Std Err:** This represents the standard error of the estimated coefficients. Lower values suggest higher precision of the coefficient estimate.\n",
        "\n",
        "**P >|t| (Pr(>|t|)):** The p-value tests the null hypothesis that the coefficient is equal to zero (no effect). A low p-value (< 0.05) indicates you can reject the null hypothesis. In other words, a predictor with a low p-value is considered to be statistically significant.\n",
        "\n",
        "**Confidence Interval:** This range captures the true coefficient value with a certain probability (typically 95%). A narrow interval indicates a more precise estimate.\n"
      ],
      "metadata": {
        "id": "6wXJMwral4pq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypothesis Testing Framework:\n",
        "**Null Hypothesis (H0):** It posits that there is no effect or no relationship between the variables. For instance, it might assert that the mean sales for different 'Outlet_Size' categories (High, Medium, Small) are equal.\n",
        "\n",
        "**Alternative Hypothesis (Ha):** This is what you want to prove, suggesting that there is an effect or a relationship. For 'Outlet_Size' and 'Item_Outlet_Sales', it might claim that the mean sales for different outlet sizes are not equal.\n",
        "\n",
        "If the p-value for a predictor is less than the significance level (often 0.05), we reject the null hypothesis in favor of the alternative. This means we have sufficient evidence to say there is a significant relationship between the predictor and the outcome variable.\n",
        "\n",
        "# Observations from Model Summary:\n",
        "Upon examining the model summary, only certain variables or specific categories of categorical variables might have a p-value lower than 0.05. These are the predictors that appear to have a statistically significant relationship with 'Item_Outlet_Sales'. It's these variables that we might consider focusing on in our model, as they have a more substantiated impact on predicting the outcome"
      ],
      "metadata": {
        "id": "pB3ZAmLXl4pq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering\n",
        "\n",
        "**Remove Multicollinearity**\n",
        "\n",
        "- Muilticollinearity occurs when two or more predictor variables (also known as independent variables) in a regression model are highly correlated, meaning they have a strong linear relationship with each other.\n",
        "\n",
        "There are different ways of detecting (or testing) multicollinearity. One such way is the Variation Inflation Factor.\n",
        "\n",
        "- Variance Inflation factor: Variance inflation factor measures the inflation in the variances of the regression parameter estimates due to collinearities that exist among the predictors. It is a measure of how much the variance of the estimated regression coefficient βk is “inflated” by the existence of correlation among the predictor variables in the model.\n",
        "\n",
        "- General Rule of thumb: If VIF is 1, then there is no correlation between the kth predictor and the remaining predictor variables, and hence the variance of β̂k is not inflated at all. Whereas, if VIF exceeds 5 or is close to exceeding 5, we say there is moderate VIF and if it is 10 or exceeds 10, it shows signs of high multicollinearity."
      ],
      "metadata": {
        "id": "U4XKr92xl4pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "vif_series = pd.Series(\n",
        "    [variance_inflation_factor(X_train_scaled.values, i) for i in range(X_train_scaled.shape[1])],\n",
        "    index = X_train_scaled.columns,\n",
        "    dtype = float)\n",
        "\n",
        "print(\"VIF Scores: \\n\\n{}\\n\".format(vif_series))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:30.002708Z",
          "iopub.execute_input": "2024-01-03T20:36:30.003632Z",
          "iopub.status.idle": "2024-01-03T20:36:30.457398Z",
          "shell.execute_reply.started": "2024-01-03T20:36:30.003586Z",
          "shell.execute_reply": "2024-01-03T20:36:30.456339Z"
        },
        "trusted": true,
        "id": "saNdwM7Nl4pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outlet_Age has a high VIF score. Hence, we are dropping Outlet_Age and building the model."
      ],
      "metadata": {
        "id": "IZ-c4KHwl4pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled_new = X_train_scaled.drop(\"Outlet_Age\", axis = 1)\n",
        "\n",
        "vif_series = pd.Series(\n",
        "    [variance_inflation_factor(X_train_scaled_new.values, i) for i in range(X_train_scaled_new.shape[1])],\n",
        "    index = X_train_scaled_new.columns,\n",
        "    dtype = float)\n",
        "\n",
        "print(\"VIF Scores: \\n\\n{}\\n\".format(vif_series))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:30.459114Z",
          "iopub.execute_input": "2024-01-03T20:36:30.460239Z",
          "iopub.status.idle": "2024-01-03T20:36:30.794176Z",
          "shell.execute_reply.started": "2024-01-03T20:36:30.460196Z",
          "shell.execute_reply": "2024-01-03T20:36:30.793113Z"
        },
        "trusted": true,
        "id": "i1EreSY7l4pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Buildiing the model and observing the p-values again\n",
        "ols_model_2 = sm.OLS(Y_train, X_train_scaled_new)\n",
        "\n",
        "ols_res_2 = ols_model_2.fit()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:30.795838Z",
          "iopub.execute_input": "2024-01-03T20:36:30.796688Z",
          "iopub.status.idle": "2024-01-03T20:36:30.816553Z",
          "shell.execute_reply.started": "2024-01-03T20:36:30.796643Z",
          "shell.execute_reply": "2024-01-03T20:36:30.815407Z"
        },
        "trusted": true,
        "id": "dsg5ATp5l4pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ols_res_2.summary())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:30.818369Z",
          "iopub.execute_input": "2024-01-03T20:36:30.819456Z",
          "iopub.status.idle": "2024-01-03T20:36:30.860736Z",
          "shell.execute_reply.started": "2024-01-03T20:36:30.819408Z",
          "shell.execute_reply": "2024-01-03T20:36:30.859579Z"
        },
        "trusted": true,
        "id": "rgYuICGbl4pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OBSERVATION:**\n",
        "When dealing with dummy variables, particularly those representing categories, it's generally not advisable to rely on Variance Inflation Factor (VIF) values to assess multicollinearity. This is because dummy variables are inherently correlated with their counterpart categories, often leading to naturally high VIF scores. Instead, a more effective approach is to examine the significance of these variables in the model, typically through their p-values.\n",
        "\n",
        "Upon building the model and analyzing the p-values, it's observed that all the categories within the 'Item_Type' column have p-values exceeding the 0.05 threshold. This suggests that they do not significantly contribute to the model's predictive power. Consequently, it's reasonable to consider removing the entire 'Item_Type' column from the model, as it appears to offer little in the way of explanatory value."
      ],
      "metadata": {
        "id": "uBibjtCJl4pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled_new2 = X_train_scaled_new.drop(['Item_Type_Breads',\n",
        "'Item_Type_Breakfast',\n",
        "'Item_Type_Canned',\n",
        "'Item_Type_Dairy',\n",
        "'Item_Type_Frozen Foods',\n",
        "'Item_Type_Fruits and Vegetables',\n",
        "'Item_Type_Hard Drinks',\n",
        "'Item_Type_Health and Hygiene',\n",
        "'Item_Type_Household',\n",
        "'Item_Type_Meat',\n",
        "'Item_Type_Others',\n",
        "'Item_Type_Seafood',\n",
        "'Item_Type_Snack Foods',\n",
        "'Item_Type_Soft Drinks',\n",
        "'Item_Type_Starchy Foods'], axis = 1)\n",
        "\n",
        "vif_series = pd.Series(\n",
        "    [variance_inflation_factor(X_train_scaled_new2.values, i) for i in range(X_train_scaled_new2.shape[1])],\n",
        "    index = X_train_scaled_new2.columns,\n",
        "    dtype = float)\n",
        "\n",
        "print(\"VIF Scores: \\n\\n{}\\n\".format(vif_series))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:30.862383Z",
          "iopub.execute_input": "2024-01-03T20:36:30.862897Z",
          "iopub.status.idle": "2024-01-03T20:36:30.93843Z",
          "shell.execute_reply.started": "2024-01-03T20:36:30.862857Z",
          "shell.execute_reply": "2024-01-03T20:36:30.937395Z"
        },
        "trusted": true,
        "id": "vtBlAwXMl4pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the Model and obsering the p-values again\n",
        "ols_model_3 = sm.OLS(Y_train, X_train_scaled_new2)\n",
        "\n",
        "ols_res_3 = ols_model_3.fit()\n",
        "\n",
        "print(ols_res_3.summary())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:30.940355Z",
          "iopub.execute_input": "2024-01-03T20:36:30.941153Z",
          "iopub.status.idle": "2024-01-03T20:36:30.978643Z",
          "shell.execute_reply.started": "2024-01-03T20:36:30.941112Z",
          "shell.execute_reply": "2024-01-03T20:36:30.977569Z"
        },
        "trusted": true,
        "id": "3uAchZo7l4pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OBSERVATION:**  Upon examining the p-values from the model, it's evident that the 'Item_Weight' column can be eliminated. This column possesses the highest p-value, indicating it is the least significant variable in terms of contributing to the model's predictive capability. Therefore, removing the 'Item_Weight' column is a justified step in refining the model and focusing on more impactful predictors."
      ],
      "metadata": {
        "id": "u9j6WWk0l4ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled_new3 = X_train_scaled_new2.drop(\"Item_Weight\", axis = 1)\n",
        "\n",
        "vif_series = pd.Series(\n",
        "    [variance_inflation_factor(X_train_scaled_new3.values, i) for i in range(X_train_scaled_new3.shape[1])],\n",
        "    index = X_train_scaled_new3.columns,\n",
        "    dtype = float)\n",
        "\n",
        "print(\"VIF Scores: \\n\\n{}\\n\".format(vif_series))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:30.980762Z",
          "iopub.execute_input": "2024-01-03T20:36:30.981585Z",
          "iopub.status.idle": "2024-01-03T20:36:31.043954Z",
          "shell.execute_reply.started": "2024-01-03T20:36:30.981543Z",
          "shell.execute_reply": "2024-01-03T20:36:31.04292Z"
        },
        "trusted": true,
        "id": "ZmO2HD90l4ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the model and observing p-values again\n",
        "ols_model_4 = sm.OLS(Y_train, X_train_scaled_new3)\n",
        "\n",
        "ols_res_4 = ols_model_4.fit()\n",
        "\n",
        "print(ols_res_4.summary())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:31.045871Z",
          "iopub.execute_input": "2024-01-03T20:36:31.046678Z",
          "iopub.status.idle": "2024-01-03T20:36:31.083217Z",
          "shell.execute_reply.started": "2024-01-03T20:36:31.046638Z",
          "shell.execute_reply": "2024-01-03T20:36:31.082168Z"
        },
        "trusted": true,
        "id": "AlbArJ1Il4ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OBSERVATION:**Based on the p-values observed, it's clear that both categories within the 'Outlet_Location_Type' column exceed the 0.05 threshold. This indicates that they do not significantly contribute to the model's performance. Consequently, it's reasonable to consider omitting the categories of 'Outlet_Location_Type' from the model due to their lack of statistical significance."
      ],
      "metadata": {
        "id": "cDwafQ6Il4ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled_new4 = X_train_scaled_new3.drop([\"Outlet_Location_Type_Tier 2\", \"Outlet_Location_Type_Tier 3\"], axis = 1)\n",
        "\n",
        "vif_series = pd.Series(\n",
        "    [variance_inflation_factor(X_train_scaled_new4.values, i) for i in range(X_train_scaled_new4.shape[1])],\n",
        "    index = X_train_scaled_new4.columns,\n",
        "    dtype = float)\n",
        "\n",
        "print(\"VIF Scores: \\n\\n{}\\n\".format(vif_series))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:31.085136Z",
          "iopub.execute_input": "2024-01-03T20:36:31.085975Z",
          "iopub.status.idle": "2024-01-03T20:36:31.134015Z",
          "shell.execute_reply.started": "2024-01-03T20:36:31.085933Z",
          "shell.execute_reply": "2024-01-03T20:36:31.13302Z"
        },
        "trusted": true,
        "id": "8NJEq00Sl4ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the model and checking the p-values\n",
        "ols_model_5 = sm.OLS(Y_train, X_train_scaled_new4)\n",
        "\n",
        "ols_res_5 = ols_model_5.fit()\n",
        "\n",
        "print(ols_res_5.summary())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:31.135732Z",
          "iopub.execute_input": "2024-01-03T20:36:31.13649Z",
          "iopub.status.idle": "2024-01-03T20:36:31.169589Z",
          "shell.execute_reply.started": "2024-01-03T20:36:31.136449Z",
          "shell.execute_reply": "2024-01-03T20:36:31.168534Z"
        },
        "trusted": true,
        "id": "4mUdsvfil4ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above p-values, both the categories of the column Outlet_Size have a p-value higher than 0.05. So, we can remove the categories of Outlet_Size."
      ],
      "metadata": {
        "id": "GJzZql73l4ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled_new5 = X_train_scaled_new4.drop( [\"Outlet_Size_Small\", \"Outlet_Size_Medium\"], axis = 1)\n",
        "\n",
        "vif_series = pd.Series(\n",
        "    [variance_inflation_factor(X_train_scaled_new4.values, i) for i in range(X_train_scaled_new4.shape[1])],\n",
        "    index = X_train_scaled_new4.columns,\n",
        "    dtype = float)\n",
        "\n",
        "print(\"VIF Scores: \\n\\n{}\\n\".format(vif_series))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:31.171415Z",
          "iopub.execute_input": "2024-01-03T20:36:31.172193Z",
          "iopub.status.idle": "2024-01-03T20:36:31.223462Z",
          "shell.execute_reply.started": "2024-01-03T20:36:31.17215Z",
          "shell.execute_reply": "2024-01-03T20:36:31.222368Z"
        },
        "trusted": true,
        "id": "qYeBW7jsl4ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the model and checking the p-values\n",
        "ols_model_6 = sm.OLS(Y_train, X_train_scaled_new5)\n",
        "\n",
        "ols_res_6 = ols_model_6.fit()\n",
        "\n",
        "print(ols_res_6.summary())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:31.22911Z",
          "iopub.execute_input": "2024-01-03T20:36:31.230002Z",
          "iopub.status.idle": "2024-01-03T20:36:31.266643Z",
          "shell.execute_reply.started": "2024-01-03T20:36:31.229957Z",
          "shell.execute_reply": "2024-01-03T20:36:31.26555Z"
        },
        "trusted": true,
        "id": "gblm6_PGl4pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, lets drop Item_visibility."
      ],
      "metadata": {
        "id": "jSZagnLvl4pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled_new6 = X_train_scaled_new5.drop( \"Item_Visibility\", axis = 1)\n",
        "\n",
        "vif_series = pd.Series(\n",
        "    [variance_inflation_factor(X_train_scaled_new5.values, i) for i in range(X_train_scaled_new5.shape[1])],\n",
        "    index = X_train_scaled_new5.columns,\n",
        "    dtype = float)\n",
        "\n",
        "print(\"VIF Scores: \\n\\n{}\\n\".format(vif_series))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:31.272079Z",
          "iopub.execute_input": "2024-01-03T20:36:31.275969Z",
          "iopub.status.idle": "2024-01-03T20:36:31.314809Z",
          "shell.execute_reply.started": "2024-01-03T20:36:31.275918Z",
          "shell.execute_reply": "2024-01-03T20:36:31.313739Z"
        },
        "trusted": true,
        "id": "Z9gIr3EDl4pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the model and checking the p-values\n",
        "ols_model_7 = sm.OLS(Y_train, X_train_scaled_new6)\n",
        "\n",
        "ols_res_7 = ols_model_7.fit()\n",
        "\n",
        "print(ols_res_7.summary())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:31.320319Z",
          "iopub.execute_input": "2024-01-03T20:36:31.321152Z",
          "iopub.status.idle": "2024-01-03T20:36:31.356092Z",
          "shell.execute_reply.started": "2024-01-03T20:36:31.321106Z",
          "shell.execute_reply": "2024-01-03T20:36:31.355028Z"
        },
        "trusted": true,
        "id": "X1iFssxul4pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OBSERVATIONS:\n",
        "- All Variance Inflation Factor (VIF) scores are now below 5, indicating the absence of multicollinearity among predictors.\n",
        "- The p-values for all remaining variables are below 0.05, signifying their statistical significance in the model.\n",
        "- The R-Squared value remains approximately 0.56, suggesting that the eliminated variables were not contributing significantly to the model's explanatory power.\n",
        "\n",
        "# Next Steps:\n",
        "We'll proceed to verify the linear regression model's assumptions. If any issues are detected, we'll address them and consider remodeling. Specifically, we'll examine:\n",
        "\n",
        "- The residuals' mean should be zero.\n",
        "- The error terms should be normally distributed.\n",
        "- There should be a linear relationship between predictors and the response variable.\n",
        "- The presence of constant variance in the residuals (homoscedasticity)."
      ],
      "metadata": {
        "id": "VNv5GN5Il4pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking residual mean\n",
        "residual = ols_res_7.resid\n",
        "residual.mean()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:31.361784Z",
          "iopub.execute_input": "2024-01-03T20:36:31.365777Z",
          "iopub.status.idle": "2024-01-03T20:36:31.378733Z",
          "shell.execute_reply.started": "2024-01-03T20:36:31.365719Z",
          "shell.execute_reply": "2024-01-03T20:36:31.377708Z"
        },
        "trusted": true,
        "id": "o9JPGIbsl4pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is almost equal to zero."
      ],
      "metadata": {
        "id": "diSh-7Zjl4pt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating Normality in Residuals:\n",
        "\n",
        "**Understanding the Test:**\n",
        "The assumption of normally distributed error terms or residuals is fundamental in linear regression analysis. This normality ensures stable confidence intervals and reliable coefficient estimates. If residuals deviate from a normal distribution, it can lead to inaccuracies in these estimates and the overall model interpretation.\n",
        "\n",
        "**Implications of Non-Normal Residuals:**\n",
        "\n",
        "When residuals are not normally distributed, it might indicate outliers or anomalies in the data. These can significantly impact the model's performance and the reliability of predictions. It's essential to identify and understand these anomalies to improve model accuracy.\n",
        "\n",
        "**Methods to Assess Normality:**\n",
        "\n",
        "- Visual Inspection: Plotting a histogram of the residuals can provide a quick and intuitive understanding of their distribution. A symmetric, bell-shaped distribution suggests normality.\n",
        "- Q-Q Plot: A Quantile-Quantile plot compares the distribution of residuals to a normal distribution. In a well-fitting model, the data points should approximately follow a straight line.\n",
        "- Shapiro-Wilk Test: This statistical test specifically checks the hypothesis of normality. A significant p-value (typically less than 0.05) would indicate the residuals are not normally distributed.\n",
        "\n",
        "**Remedial Measures for Non-Normal Residuals:**\n",
        "\n",
        "If the residuals are found to be non-normal, various transformations can be considered to normalize them. Common transformations include logarithmic, exponential, and arcsine transformations. The choice of transformation depends on the specific pattern of non-normality and the underlying data distribution. Applying these transformations can help stabilize variance and make the model more robust and reliable.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Tz8aAercl4pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histogram of residuals for checking normal distribution\n",
        "sns.histplot(residual, kde = True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:31.383167Z",
          "iopub.execute_input": "2024-01-03T20:36:31.3874Z",
          "iopub.status.idle": "2024-01-03T20:36:31.875515Z",
          "shell.execute_reply.started": "2024-01-03T20:36:31.387349Z",
          "shell.execute_reply": "2024-01-03T20:36:31.87475Z"
        },
        "trusted": true,
        "id": "ROdUVUVdl4pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the graph seems normally distributed."
      ],
      "metadata": {
        "id": "V_zggIw3l4pt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assessing the Linearity Assumption:\n",
        "\n",
        "The linearity assumption in regression analysis posits a direct, linear relationship between the predictor(s) and the dependent variable. This means that changes in the predictor variables are expected to result in proportional changes in the response variable.\n",
        "\n",
        "**How to Validate Linearity:**\n",
        "\n",
        "**Residuals vs. Fitted Values Plot:** One common method to check for linearity is to plot the residuals (the differences between the observed and predicted values) against the predicted (fitted) values. If the assumption of linearity holds, the residuals should display no discernible pattern when plotted against the fitted values. They should be evenly dispersed across the range of fitted values, indicating that the relationship is linear across all values of the predictor variables.\n",
        "\n",
        "\n",
        "In essence, a lack of clear pattern or structure in this plot suggests that the linearity assumption is reasonable for the data at hand. Any apparent pattern, curve, or systematic structure in the plot would suggest a violation of linearity, indicating that the model may benefit from the inclusion of non-linear terms or a transformation of variables."
      ],
      "metadata": {
        "id": "G3Vm4EcQl4pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicted values check for linearity\n",
        "fitted = ols_res_7.fittedvalues\n",
        "\n",
        "sns.residplot(x = fitted, y = residual, color = \"lightblue\")\n",
        "\n",
        "plt.xlabel(\"Fitted Values\")\n",
        "\n",
        "plt.ylabel(\"Residual\")\n",
        "\n",
        "plt.title(\"Residual PLOT\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:31.87699Z",
          "iopub.execute_input": "2024-01-03T20:36:31.878053Z",
          "iopub.status.idle": "2024-01-03T20:36:32.184162Z",
          "shell.execute_reply.started": "2024-01-03T20:36:31.878009Z",
          "shell.execute_reply": "2024-01-03T20:36:32.183109Z"
        },
        "trusted": true,
        "id": "VpCKw4KIl4pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OBSERVATION:**\n",
        "\n",
        "The plot of residuals versus fitted values reveals a discernible pattern, indicating that the residuals aren't randomly dispersed. This suggests a potential violation of the linearity assumption.\n",
        "\n",
        "**Proposed Solution:**\n",
        "\n",
        "To address this issue, one approach is to apply a log transformation to the target variable. This transformation can often stabilize variance and linearize relationships, making the data more suitable for linear regression modeling. After performing the log transformation, we'll rebuild the model and reassess the residuals to determine if this adjustment leads to an improvement in meeting the linearity assumption."
      ],
      "metadata": {
        "id": "78BcdIUBl4pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Log transformation on the target variable\n",
        "import numpy as np\n",
        "Y_train_log = np.log(Y_train)\n",
        "\n",
        "# Fitting new model with the transformed target variable\n",
        "ols_model_7 = sm.OLS(Y_train_log, X_train_scaled_new6)\n",
        "\n",
        "ols_res_7 = ols_model_7.fit()\n",
        "\n",
        "# Predicted values\n",
        "fitted = ols_res_7.fittedvalues\n",
        "\n",
        "residual1 = ols_res_7.resid\n",
        "\n",
        "sns.residplot(x = fitted, y = residual1, color = \"lightblue\")\n",
        "\n",
        "plt.xlabel(\"Fitted Values\")\n",
        "\n",
        "plt.ylabel(\"Residual\")\n",
        "\n",
        "plt.title(\"Residual PLOT\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:32.185556Z",
          "iopub.execute_input": "2024-01-03T20:36:32.185948Z",
          "iopub.status.idle": "2024-01-03T20:36:32.50515Z",
          "shell.execute_reply.started": "2024-01-03T20:36:32.185911Z",
          "shell.execute_reply": "2024-01-03T20:36:32.504102Z"
        },
        "trusted": true,
        "id": "xyndvltal4pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations** indicate an absence of discernible patterns in the scatter plot of residuals versus fitted values, suggesting that the linearity assumption is now met. With this encouraging result, the next step is to review the summary statistics of the newly fitted model to evaluate its performance and the significance of the predictors."
      ],
      "metadata": {
        "id": "yGzX90XBl4pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#printing the summary\n",
        "print(ols_res_7.summary())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:32.506559Z",
          "iopub.execute_input": "2024-01-03T20:36:32.507295Z",
          "iopub.status.idle": "2024-01-03T20:36:32.534089Z",
          "shell.execute_reply.started": "2024-01-03T20:36:32.507264Z",
          "shell.execute_reply": "2024-01-03T20:36:32.533024Z"
        },
        "trusted": true,
        "id": "76OPa-Pfl4pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the final assumption of homoscedasticity."
      ],
      "metadata": {
        "id": "iEWocK1Xl4pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating Homoscedasticity:\n",
        "\n",
        "**Understanding Homoscedasticity:**\n",
        "Homoscedasticity refers to a condition where the residuals' variance is consistent across all levels of the independent variables. It's a crucial assumption in regression analysis, ensuring that the model's predictive accuracy and confidence intervals remain reliable across the entire range of data.\n",
        "\n",
        "**Identifying Heteroscedasticity:**\n",
        "Conversely, heteroscedasticity occurs when the variance of residuals fluctuates across the values of the independent variables. This can manifest as a fan shape or other irregular patterns in the plot of residuals, undermining the reliability of standard errors and test statistics.\n",
        "\n",
        "**Goldfeld–Quandt Test:**\n",
        "To formally test for homoscedasticity, we'll employ the Goldfeld-Quandt test. This test compares the variance of residuals in two subsets of the data:\n",
        "\n",
        "**Null Hypothesis (H0):** The residuals are homoscedastic, meaning the variance is the same across the data.\n",
        "\n",
        "\n",
        "**Alternative Hypothesis (H1):** The residuals are heteroscedastic, indicating a variance that changes with the independent variables.\n",
        "\n",
        "By conducting this test, we'll be able to statistically determine whether the variance of the residuals is constant, satisfying the assumption of homoscedasticity, or if it's variable, indicating potential heteroscedasticity that might necessitate further investigation or model adjustments."
      ],
      "metadata": {
        "id": "RLTGTOh3l4pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.diagnostic import het_white\n",
        "\n",
        "from statsmodels.compat import lzip\n",
        "\n",
        "import statsmodels.stats.api as sms\n",
        "\n",
        "from statsmodels.compat import lzip\n",
        "\n",
        "name = [\"F statistic\", \"p-value\"]\n",
        "\n",
        "test = sms.het_goldfeldquandt(Y_train_log, X_train_scaled_new6)\n",
        "\n",
        "lzip(name, test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T20:36:32.535778Z",
          "iopub.execute_input": "2024-01-03T20:36:32.536397Z",
          "iopub.status.idle": "2024-01-03T20:36:32.555314Z",
          "shell.execute_reply.started": "2024-01-03T20:36:32.536354Z",
          "shell.execute_reply": "2024-01-03T20:36:32.554288Z"
        },
        "trusted": true,
        "id": "Lcj98edEl4pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results of the Goldfeld-Quandt test, the p-value exceeds 0.05, leading us to retain the null hypothesis. This suggests the residuals are homoscedastic, meaning the variance is consistent across the regression line. With all assumptions of the linear regression model now verified, we can confidently proceed with the final model. The derived equation is:\n",
        "\n",
        "**log(Item_Outlet_Sales)**=4.6356+1.9555⋅**Item_MRP**+0.0158⋅**Item_Fat_Content_Regular**+1.9550⋅**Outlet_Type_SupermarketType1**+1.7737⋅**Outlet_Type_SupermarketType2**+2.4837⋅**Outlet_Type_SupermarketTyp3**\n",
        "\n",
        "With this model equation established, we're ready to move forward and utilize it for making predictions on the test data."
      ],
      "metadata": {
        "id": "SfD6phTYl4pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Applying transform on the test data\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns = without_const.columns)\n",
        "\n",
        "X_test_features = sm.add_constant(X_test_scaled)\n",
        "\n",
        "X_test_scaled = X_test_scaled.drop([\"Item_Weight\", \"Item_Visibility\", \"Item_Type_Breads\", \"Item_Type_Breakfast\", \"Item_Type_Canned\", \"Item_Type_Dairy\",\"Item_Type_Frozen Foods\",\"Item_Type_Fruits and Vegetables\", \"Item_Type_Hard Drinks\", \"Item_Type_Health and Hygiene\", \"Item_Type_Household\", \"Item_Type_Meat\", \"Item_Type_Others\", \"Item_Type_Seafood\", \"Item_Type_Snack Foods\", \"Item_Type_Soft Drinks\", \"Item_Type_Starchy Foods\", \"Outlet_Size_Medium\", \"Outlet_Size_Small\", \"Outlet_Location_Type_Tier 2\", \"Outlet_Location_Type_Tier 3\", 'Outlet_Age'], axis = 1)\n",
        "\n",
        "X_test_scaled.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T21:03:33.75476Z",
          "iopub.execute_input": "2024-01-03T21:03:33.755155Z",
          "iopub.status.idle": "2024-01-03T21:03:33.787597Z",
          "shell.execute_reply.started": "2024-01-03T21:03:33.755116Z",
          "shell.execute_reply": "2024-01-03T21:03:33.786435Z"
        },
        "trusted": true,
        "id": "QYzSBLgtl4pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metrics:\n",
        "\n",
        "**R-Squared Evaluation:**\n",
        "The R-squared value serves as a gauge for the model's effectiveness relative to a basic baseline model that doesn't incorporate any independent variables. In this context, our model accounts for approximately 98% of the variance in the data, significantly outperforming the baseline and indicating a high level of explanatory power."
      ],
      "metadata": {
        "id": "Qxg0fpDLl4pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ols_res_7.rsquared"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T21:04:09.377475Z",
          "iopub.execute_input": "2024-01-03T21:04:09.377877Z",
          "iopub.status.idle": "2024-01-03T21:04:09.383286Z",
          "shell.execute_reply.started": "2024-01-03T21:04:09.377845Z",
          "shell.execute_reply": "2024-01-03T21:04:09.382567Z"
        },
        "trusted": true,
        "id": "OZfUzuall4pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Squared Error (MSE) Analysis:**\n",
        "MSE is a metric that quantifies the average of the squared discrepancies between predicted values and the actual observed values. It essentially provides a mean of the squared errors, offering insight into the overall error magnitude of the model's predictions."
      ],
      "metadata": {
        "id": "iEeF7hDPl4pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ols_res_7.mse_resid"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T21:04:14.934085Z",
          "iopub.execute_input": "2024-01-03T21:04:14.934509Z",
          "iopub.status.idle": "2024-01-03T21:04:14.941958Z",
          "shell.execute_reply.started": "2024-01-03T21:04:14.934476Z",
          "shell.execute_reply": "2024-01-03T21:04:14.94091Z"
        },
        "trusted": true,
        "id": "hAJ5BRI2l4pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Root Mean Squared Error (RMSE):**\n",
        "RMSE is an extension of the Mean Squared Error (MSE) metric. By taking the square root of MSE, RMSE is obtained, which scales the error back to the units of the target variable. This adjustment makes the metric more interpretable, providing a clearer understanding of the model's prediction error in relation to the actual values."
      ],
      "metadata": {
        "id": "O8AS26wal4pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.sqrt(ols_res_7.mse_resid)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T21:04:20.40853Z",
          "iopub.execute_input": "2024-01-03T21:04:20.40926Z",
          "iopub.status.idle": "2024-01-03T21:04:20.416159Z",
          "shell.execute_reply.started": "2024-01-03T21:04:20.409224Z",
          "shell.execute_reply": "2024-01-03T21:04:20.415134Z"
        },
        "trusted": true,
        "id": "zoPozuKjl4pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following step, we'll assess the model's performance using cross-validation. This technique helps determine whether the model is underfitting, overfitting, or appropriately fitted to the data by evaluating its predictive accuracy across different subsets."
      ],
      "metadata": {
        "id": "YaJARyuPl4pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting linear model\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "linearregression = LinearRegression()\n",
        "\n",
        "cv_Score11 = cross_val_score(linearregression, X_train_scaled_new6, Y_train_log, cv = 10)\n",
        "\n",
        "cv_Score12 = cross_val_score(linearregression, X_train_scaled_new6, Y_train_log, cv = 10,\n",
        "                             scoring = 'neg_mean_squared_error')\n",
        "\n",
        "\n",
        "print(\"RSquared: %0.3f (+/- %0.3f)\" % (cv_Score11.mean(), cv_Score11.std()*2))\n",
        "\n",
        "print(\"Mean Squared Error: %0.3f (+/- %0.3f)\" % (-1*cv_Score12.mean(), cv_Score12.std()*2))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T21:04:29.102392Z",
          "iopub.execute_input": "2024-01-03T21:04:29.103117Z",
          "iopub.status.idle": "2024-01-03T21:04:29.234706Z",
          "shell.execute_reply.started": "2024-01-03T21:04:29.103061Z",
          "shell.execute_reply": "2024-01-03T21:04:29.233668Z"
        },
        "trusted": true,
        "id": "srhAoWEel4pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "- The cross-validation R-Squared value is 0.718, closely aligning with the training dataset's R-Squared. This consistency suggests the model is neither overfitting nor underfitting but is rather well-suited to the data.\n",
        "- Similarly, the Mean Squared Error (MSE) from cross-validation is 0.290, mirroring the training dataset's performance and further supporting the model's appropriate fit.\n",
        "- The overall evidence indicates that our model is well-calibrated, offering a generalized performance across different data subsets.\n",
        "- Despite the model's satisfactory performance, it's worth noting that as a linear model, it might not fully capture potential nonlinear patterns in the data. Exploring more advanced regression techniques could potentially unveil and leverage these patterns, potentially enhancing the model's predictive power. However, delving into such complex models falls beyond this case study's scope"
      ],
      "metadata": {
        "id": "4De1k87Kl4pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the no. of features in X_test_scaled\n",
        "X_test_scaled.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T21:10:03.109404Z",
          "iopub.execute_input": "2024-01-03T21:10:03.109809Z",
          "iopub.status.idle": "2024-01-03T21:10:03.122654Z",
          "shell.execute_reply.started": "2024-01-03T21:10:03.109775Z",
          "shell.execute_reply": "2024-01-03T21:10:03.121647Z"
        },
        "trusted": true,
        "id": "YkyCsau7l4pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the number of features in X_train_Scaled_new6\n",
        "X_train_scaled_new6.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T21:10:09.579825Z",
          "iopub.execute_input": "2024-01-03T21:10:09.580236Z",
          "iopub.status.idle": "2024-01-03T21:10:09.594446Z",
          "shell.execute_reply.started": "2024-01-03T21:10:09.580204Z",
          "shell.execute_reply": "2024-01-03T21:10:09.593423Z"
        },
        "trusted": true,
        "id": "Rjiwgf-Kl4pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#As we have observed above there is no const column in X_test_scaled column so we are adding it\n",
        "from statsmodels.api import add_constant\n",
        "X_test_scaled_with_const = add_constant(X_test_scaled)\n",
        "#These predictions will be on log scale\n",
        "test_predictions = ols_res_7.predict(X_test_scaled_with_const)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T21:07:17.943566Z",
          "iopub.execute_input": "2024-01-03T21:07:17.943979Z",
          "iopub.status.idle": "2024-01-03T21:07:17.956917Z",
          "shell.execute_reply.started": "2024-01-03T21:07:17.943945Z",
          "shell.execute_reply": "2024-01-03T21:07:17.955676Z"
        },
        "trusted": true,
        "id": "PFntVzivl4pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We are converting the log scale predictions to its original scale\n",
        "test_predictions_inverse_transformed = np.exp(test_predictions)\n",
        "\n",
        "test_predictions_inverse_transformed"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T21:07:40.79248Z",
          "iopub.execute_input": "2024-01-03T21:07:40.793212Z",
          "iopub.status.idle": "2024-01-03T21:07:40.801204Z",
          "shell.execute_reply.started": "2024-01-03T21:07:40.793176Z",
          "shell.execute_reply": "2024-01-03T21:07:40.800165Z"
        },
        "trusted": true,
        "id": "btfzRWqll4pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visulaizing the predictions in both log scale and inverse scale\n",
        "fig, ax = plt.subplots(1, 2, figsize = (24, 12))\n",
        "\n",
        "sns.histplot(test_predictions, ax = ax[0]);\n",
        "\n",
        "sns.histplot(test_predictions_inverse_transformed, ax = ax[1]);"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-03T21:11:57.298349Z",
          "iopub.execute_input": "2024-01-03T21:11:57.298761Z",
          "iopub.status.idle": "2024-01-03T21:11:57.996954Z",
          "shell.execute_reply.started": "2024-01-03T21:11:57.298717Z",
          "shell.execute_reply": "2024-01-03T21:11:57.99594Z"
        },
        "trusted": true,
        "id": "ZJJ0JM8vl4pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion & Recommendations:\n",
        "\n",
        "Throughout this analysis, we embarked on a comprehensive journey through Exploratory Data Analysis (EDA), examining each variable individually and in relation to others. We addressed missing values with informed strategies based on the relationships between variables.\n",
        "\n",
        "The modeling process began with a full set of features, from which we methodically eliminated those contributing to multicollinearity and those deemed statistically insignificant. We rigorously tested the linear regression model against its underlying assumptions, refining it iteratively to ensure its validity.\n",
        "\n",
        "Our evaluation encompassed various metrics, culminating in a robust and interpretable model. The final model equation is as follows:\n",
        "\n",
        "log(Item_Outlet_Sales)=4.6356+1.9555⋅Item_MRP+0.0158⋅Item_Fat_Content_Regular+∑(Outlet Type Coefficients)log(Item_Outlet_Sales)=4.6356+1.9555⋅Item_MRP+0.0158⋅Item_Fat_Content_Regular+∑(Outlet Type Coefficients)\n",
        "\n",
        "# Key Interpretations:\n",
        "\n",
        "**Item_MRP's Impact:** A one-unit increase in Item_MRP is associated with a 1.9555 unit increase in the log of Item_Outlet_Sales. Strategically, positioning higher MRP items in prominent areas could potentially drive sales.\n",
        "\n",
        "**Supermarket Type 3's Superiority:** Compared to other types, Supermarket Type 3 stores exhibit significantly higher sales. Specifically, their log sales are approximately 1.4 times those of Type 2 and 1.27 times those of Type 1 stores, when other factors are constant.\n",
        "\n",
        "# Strategic Recommendations:\n",
        "\n",
        "**Enhance Type 3 Stores:** Given their strong performance, maintaining or further enhancing the appeal of Type 3 supermarkets should be a priority.\n",
        "\n",
        "\n",
        "**Boost Other Stores' Sales:** For other types, focus on strategies like improving customer service, offering staff training, and increasing the visibility of high MRP items.\n",
        "\n",
        "\n",
        "**Customized Approaches:** Different strategies may be needed for each store type, considering their unique characteristics and customer base.\n",
        "\n",
        "\n",
        "In summary, this analysis has not only provided a predictive model but also insights that can inform targeted strategies to optimize sales across different store types."
      ],
      "metadata": {
        "id": "XHNdQKwUl4pv"
      }
    }
  ]
}